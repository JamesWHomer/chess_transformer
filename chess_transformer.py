"""chess_transformer.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Bqh7OZQ835LwboKhiXMsa7FlDFYyfBoe
"""

import pandas as pd
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader, IterableDataset
import numpy as np
from typing import List, Optional, Iterator
import math
import random
from tqdm import tqdm
import json
import queue
import threading
import time
import matplotlib.pyplot as plt
import os

# Model Configuration
VOCAB_SIZE = 128
D_MODEL = 512      # Model size
N_HEADS = 8        # Heads
N_LAYERS = 6       # Layers
BATCH_SIZE = 16    # Batch size
LEARNING_RATE = 1e-4  # Learning rate
DROPOUT = 0.1      # Dropout
CONTEXT_LENGTH = 1024  # Increased context length
NUM_EPOCHS = 2    # Number of epochs to train
QUEUE_SIZE = 100   # Size of the queue for batches
NUM_WORKERS = 4    # Number of tokenization workers

# Special tokens for game structure
SPECIAL_TOKENS = {
    '<PAD>': 0,    # Padding token for batching sequences of different lengths
    '<START>': 1,   # Indicates start of game
    '<WHITE>': 2,   # Indicates White's turn
    '<BLACK>': 3,   # Indicates Black's turn
    '<WHITE_WINS>': 4,  # Game outcome tokens
    '<BLACK_WINS>': 5,
    '<DRAW>': 6
}

#######################
# Dataset and Tokenizer
#######################

class ChessTokenizer:
    def __init__(self):
        self.token_to_id = SPECIAL_TOKENS.copy()
        self.id_to_token = {v: k for k, v in self.token_to_id.items()}
        self._build_vocabulary()
        # Sort tokens by length for proper tokenization of longer tokens first
        self.sorted_tokens = sorted(self.token_to_id.keys(), key=len, reverse=True)

    def _build_vocabulary(self):
        # Define all possible tokens systematically
        pieces = ['P', 'N', 'B', 'R', 'K', 'Q']
        files = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h']
        ranks = ['1', '2', '3', '4', '5', '6', '7', '8']
        squares = [f + r for f in files for r in ranks]
        actions = ['m', 'x']  # Basic move and capture
        special_moves = ['O-O', 'O-O-O']  # Castling
        modifiers = ['=', '+', '#']  # Promotion, check, checkmate
        promotion_pieces = ['N', 'B', 'R', 'Q']  # Pieces that pawns can promote to

        # Build vocabulary systematically
        vocab = (pieces + squares + actions + special_moves + 
                modifiers + ['='+p for p in promotion_pieces])

        # Verify vocabulary completeness
        test_tokens = {
            'Pe2me4',  # Basic pawn move
            'Ng1mf3',  # Piece move
            'Pe4xd5',  # Capture
            'O-O',     # Kingside castle
            'O-O-O',   # Queenside castle
            'Pe7me8=Q',# Promotion
            'Ke1me2+', # Check
            'Ke1me2#'  # Checkmate
        }
        
        # Add tokens to vocabulary with unique IDs
        current_id = len(self.token_to_id)
        for token in vocab:
            if token not in self.token_to_id:
                self.token_to_id[token] = current_id
                self.id_to_token[current_id] = token
                current_id += 1

    def encode(self, text: str) -> List[int]:
        """Encode a chess game text into token IDs."""
        if not text:
            return []

        tokens = []
        i = 0
        text_len = len(text)
        
        while i < text_len:
            matched = False
            # Try to match special tokens first
            for token in self.sorted_tokens:
                if text[i:].startswith(token):
                    tokens.append(self.token_to_id[token])
                    i += len(token)
                    matched = True
                    break
            
            if not matched:
                # Instead of silently skipping, maybe log unknown characters
                print(f"Warning: Unknown character '{text[i]}' at position {i}")
                i += 1
                continue

        return tokens

    def decode(self, ids: List[int]) -> str:
        """Decode a list of token IDs back into a chess game text."""
        return ''.join(self.id_to_token.get(id, '') for id in ids)

    def encode_with_special_tokens(self, text: str) -> List[int]:
        """Encode text and add special tokens as needed."""
        tokens = self.encode(text)
        if not tokens or tokens[0] != self.token_to_id['<START>']:
            tokens.insert(0, self.token_to_id['<START>'])
        return tokens

    def __len__(self):
        """Return the size of the vocabulary."""
        return len(self.token_to_id)

class DataQueue:
    def __init__(self, maxsize: int = QUEUE_SIZE):
        self.queue = queue.Queue(maxsize=maxsize)
        self.shutdown = False

    def put(self, item):
        if not self.shutdown:
            self.queue.put(item)

    def get(self):
        if self.shutdown and self.queue.empty():
            raise StopIteration
        return self.queue.get()

    def stop(self):
        self.shutdown = True

class StreamingChessDataset(IterableDataset):
    def __init__(self, games: List[str], tokenizer: ChessTokenizer, queue: DataQueue = None):
        self.games = games
        self.total_samples = len(games)  # Store total number of games
        self.tokenizer = tokenizer
        self.context_length = CONTEXT_LENGTH
        self.queue = queue if queue is not None else DataQueue()
        self.worker_info = None

    def get_total_samples(self):
        """Return the total number of samples in the dataset"""
        return self.total_samples

    def tokenize_game(self, game: str) -> Optional[torch.Tensor]:
        try:
            encoded = self.tokenizer.encode_with_special_tokens(game)
            if len(encoded) > 4:  # Minimum length check
                if len(encoded) <= self.context_length:
                    return torch.tensor(encoded)
                else:
                    return torch.tensor(encoded[:self.context_length])
        except Exception as e:
            print(f"Error encoding game: {str(e)}")
        return None

    def producer(self, start_idx: int, end_idx: int):
        """Producer function that tokenizes games and puts them in the queue"""
        try:
            for idx in range(start_idx, end_idx):
                if self.queue.shutdown:
                    break
                game = self.games[idx]
                tokens = self.tokenize_game(game)
                if tokens is not None:
                    self.queue.put((tokens[:-1], tokens[1:]))
        except Exception as e:
            print(f"Producer error: {str(e)}")
        finally:
            self.queue.stop()

    def __iter__(self) -> Iterator:
        # Get worker info for multi-process data loading
        worker_info = torch.utils.data.get_worker_info()
        
        if worker_info is None:  # Single worker
            start_idx = 0
            end_idx = len(self.games)
        else:  # Multiple workers
            per_worker = len(self.games) // worker_info.num_workers
            worker_id = worker_info.id
            start_idx = worker_id * per_worker
            end_idx = start_idx + per_worker if worker_id < worker_info.num_workers - 1 else len(self.games)

        # Start producer thread
        producer_thread = threading.Thread(
            target=self.producer,
            args=(start_idx, end_idx)
        )
        producer_thread.daemon = True
        producer_thread.start()

        # Consumer loop
        while True:
            try:
                item = self.queue.get()
                yield item
            except StopIteration:
                break
            except Exception as e:
                print(f"Consumer error: {str(e)}")
                break

def collate_fn(batch):
    inputs, targets = zip(*batch)
    max_len = max(len(x) for x in inputs)
    
    padded_inputs = torch.zeros((len(batch), max_len), dtype=torch.long)
    padded_targets = torch.zeros((len(batch), max_len), dtype=torch.long)
    
    for i, (input, target) in enumerate(zip(inputs, targets)):
        padded_inputs[i, :len(input)] = input
        padded_targets[i, :len(target)] = target
    
    # Create padding mask (True where tokens are padded)
    padding_mask = padded_inputs == SPECIAL_TOKENS['<PAD>']
    
    return padded_inputs, padded_targets, padding_mask

def create_dataloaders(games: List[str], tokenizer: ChessTokenizer, val_split: float = 0.1):
    # Split games into train and validation
    split_idx = int(len(games) * (1 - val_split))
    train_games = games[:split_idx]
    val_games = games[split_idx:]
    
    # Create datasets with separate queues
    train_queue = DataQueue()
    val_queue = DataQueue()
    train_dataset = StreamingChessDataset(train_games, tokenizer, train_queue)
    val_dataset = StreamingChessDataset(val_games, tokenizer, val_queue)
    
    # Create dataloaders with multiple workers for training
    train_loader = DataLoader(
        train_dataset,
        batch_size=BATCH_SIZE,
        num_workers=NUM_WORKERS,
        collate_fn=collate_fn
    )
    # Single worker for validation to maintain order
    val_loader = DataLoader(
        val_dataset,
        batch_size=BATCH_SIZE,
        num_workers=1,
        collate_fn=collate_fn
    )
    
    return train_loader, val_loader, train_dataset.get_total_samples(), val_dataset.get_total_samples()

#######################
# Model Architecture
#######################

class PositionalEmbedding(nn.Module):
    def __init__(self, d_model: int, max_seq_length: int = CONTEXT_LENGTH):
        super().__init__()
        self.pos_embedding = nn.Embedding(max_seq_length, d_model)
        
    def forward(self, x):
        positions = torch.arange(x.size(1), device=x.device).unsqueeze(0)
        pos_embed = self.pos_embedding(positions)
        return x + pos_embed

class DecoderBlock(nn.Module):
    def __init__(self, d_model: int, n_heads: int, dropout: float = 0.1):
        super().__init__()
        self.ln1 = nn.LayerNorm(d_model)
        self.ln2 = nn.LayerNorm(d_model)
        
        self.self_attn = nn.MultiheadAttention(
            embed_dim=d_model,
            num_heads=n_heads,
            dropout=dropout,
            batch_first=True
        )
        
        self.mlp = nn.Sequential(
            nn.Linear(d_model, 4 * d_model),
            nn.GELU(),
            nn.Linear(4 * d_model, d_model),
            nn.Dropout(dropout)
        )
        
        self.dropout = nn.Dropout(dropout)

    def forward(self, x, causal_mask=None, padding_mask=None):
        # Pre-norm architecture
        normed_x = self.ln1(x)
        
        # Convert causal_mask to boolean mask if it exists
        if causal_mask is not None:
            causal_mask = causal_mask == float('-inf')
            
        attn_output, _ = self.self_attn(
            normed_x, normed_x, normed_x,
            attn_mask=causal_mask,
            key_padding_mask=padding_mask,
            need_weights=False
        )
        x = x + self.dropout(attn_output)
        
        normed_x = self.ln2(x)
        mlp_output = self.mlp(normed_x)
        x = x + mlp_output
        
        return x

class ChessTransformer(nn.Module):
    def __init__(self, vocab_size, d_model, n_heads, n_layers, dropout):
        super().__init__()
        self.token_embedding = nn.Embedding(vocab_size, d_model)
        self.pos_encoder = PositionalEmbedding(d_model)
        
        self.decoder_layers = nn.ModuleList([
            DecoderBlock(d_model, n_heads, dropout)
            for _ in range(n_layers)
        ])
        
        self.ln_f = nn.LayerNorm(d_model)  # Final layer norm
        self.fc_out = nn.Linear(d_model, vocab_size)
        
        # Tie weights between input embedding and output layer
        self.fc_out.weight = self.token_embedding.weight
        
    def get_causal_mask(self, seq_len: int, device):
        # Create causal mask for the sequence
        mask = torch.triu(torch.ones(seq_len, seq_len, device=device) * float('-inf'), diagonal=1)
        return mask

    def forward(self, x, padding_mask=None):
        x = self.token_embedding(x)
        x = self.pos_encoder(x)
        
        # Create causal attention mask
        causal_mask = self.get_causal_mask(x.size(1), x.device)
        
        # Apply decoder layers with both causal and padding masks
        for layer in self.decoder_layers:
            x = layer(x, causal_mask=causal_mask, padding_mask=padding_mask)
        
        x = self.ln_f(x)
        logits = self.fc_out(x)
        
        return logits


#######################
# Training Functions
#######################

def plot_training_history(train_losses, val_losses, batch_indices, eval_every_n_batches, save_path='training_history.png'):
    """Plot and save the training and validation loss curves with batch-level granularity."""
    # Create a figure with two subplots side by side
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 6))
    
    # Calculate smoothed training loss using exponential moving average
    alpha = 0.1  # Smoothing factor
    smoothed_losses = []
    curr_smoothed = train_losses[0]
    for loss in train_losses:
        curr_smoothed = alpha * loss + (1 - alpha) * curr_smoothed
        smoothed_losses.append(curr_smoothed)
    
    # Plot 1: Full training history with log scale
    ax1.semilogy(batch_indices, train_losses, label='Training Loss', alpha=0.3)
    ax1.semilogy(batch_indices, smoothed_losses, label='Smoothed Training Loss', linewidth=2)
    val_indices = [i * eval_every_n_batches for i in range(len(val_losses))]
    ax1.semilogy(val_indices, val_losses, label='Validation Loss', linewidth=2, marker='o')
    
    # Customize log scale ticks
    ax1.yaxis.set_major_formatter(plt.ScalarFormatter())
    ax1.set_xlabel('Batch')
    ax1.set_ylabel('Loss (log scale)')
    ax1.set_title('Full Training History (Log Scale)')
    ax1.legend()
    ax1.grid(True, which="both", ls="-", alpha=0.2)
    
    # Plot 2: Recent batches with linear scale
    recent_window = 500  # Show last 500 batches
    if len(train_losses) > recent_window:
        recent_train = train_losses[-recent_window:]
        recent_smoothed = smoothed_losses[-recent_window:]
        recent_batch_indices = batch_indices[-recent_window:]
        
        # Find validation losses that fall within the recent window
        start_batch = batch_indices[-recent_window]
        recent_val_indices = [i for i in val_indices if i >= start_batch]
        recent_val_losses = val_losses[-len(recent_val_indices):]
        
        ax2.plot(recent_batch_indices, recent_train, label='Training Loss', alpha=0.3)
        ax2.plot(recent_batch_indices, recent_smoothed, label='Smoothed Training Loss', linewidth=2)
        if recent_val_indices:
            ax2.plot(recent_val_indices, recent_val_losses, label='Validation Loss', linewidth=2, marker='o')
    else:
        ax2.plot(batch_indices, train_losses, label='Training Loss', alpha=0.3)
        ax2.plot(batch_indices, smoothed_losses, label='Smoothed Training Loss', linewidth=2)
        ax2.plot(val_indices, val_losses, label='Validation Loss', linewidth=2, marker='o')
    
    ax2.set_xlabel('Batch')
    ax2.set_ylabel('Loss (linear scale)')
    ax2.set_title('Recent Training History (Linear Scale)')
    ax2.legend()
    ax2.grid(True, alpha=0.2)
    
    plt.tight_layout()
    plt.savefig(save_path)
    plt.close()

def train_model(model, train_loader, val_loader, optimizer, device, eval_every_n_batches=50):
    model.train()
    total_loss = 0
    num_batches = 0
    train_losses = []
    val_losses = []
    batch_indices = []
    current_batch = 0
    
    # For calculating recent average loss
    recent_window = 100

    # Calculate total batches for progress bar
    total_samples = train_loader.dataset.get_total_samples()
    total_batches = (total_samples + train_loader.batch_size - 1) // train_loader.batch_size

    pbar = tqdm(train_loader, total=total_batches, desc="Training batches")
    for data, target, padding_mask in pbar:
        data, target = data.to(device), target.to(device)
        padding_mask = padding_mask.to(device)
        optimizer.zero_grad()

        output = model(data, padding_mask=padding_mask)
        loss = F.cross_entropy(output.view(-1, output.size(-1)), target.view(-1), ignore_index=0)
        
        loss.backward()
        optimizer.step()

        total_loss += loss.item()
        num_batches += 1
        current_batch += 1
        
        # Store training loss
        train_losses.append(loss.item())
        batch_indices.append(current_batch)
        
        # Calculate recent average loss
        recent_train_loss = sum(train_losses[-recent_window:]) / len(train_losses[-recent_window:])
        
        # Evaluate on validation set periodically
        if current_batch % eval_every_n_batches == 0:
            val_loss = evaluate_model(model, val_loader, device, max_batches=10)  # Limit validation batches for speed
            val_losses.append(val_loss)
            pbar.set_postfix({
                'recent_train_loss': f'{recent_train_loss:.4f}',
                'val_loss': f'{val_loss:.4f}'
            })
            
            # Plot current progress
            plot_training_history(
                train_losses, 
                val_losses, 
                batch_indices, 
                eval_every_n_batches,
                save_path='training_history.png'
            )
        else:
            pbar.set_postfix({'recent_train_loss': f'{recent_train_loss:.4f}'})

    # Calculate final metrics using recent windows
    final_train_loss = sum(train_losses[-recent_window:]) / len(train_losses[-recent_window:])
    return final_train_loss, train_losses, val_losses, batch_indices

def evaluate_model(model, val_loader, device, max_batches=None):
    model.eval()
    total_loss = 0
    num_batches = 0
    
    with torch.no_grad():
        for data, target, padding_mask in val_loader:
            if max_batches and num_batches >= max_batches:
                break
                
            data, target = data.to(device), target.to(device)
            padding_mask = padding_mask.to(device)
            output = model(data, padding_mask=padding_mask)
            loss = F.cross_entropy(output.view(-1, output.size(-1)), target.view(-1), ignore_index=0)
            total_loss += loss.item()
            num_batches += 1
    
    return total_loss / num_batches

def generate_move(model, tokenizer, input_sequence: str, temperature: float = 1.0):
    """Generate next moves in a chess game.
    
    Args:
        model: The transformer model
        tokenizer: The chess tokenizer
        input_sequence: The game sequence so far
        temperature: Temperature for sampling (1.0 = normal, <1.0 = conservative, >1.0 = creative)
    """
    model.eval()
    device = next(model.parameters()).device
    input_ids = torch.tensor(tokenizer.encode(input_sequence)).unsqueeze(0).to(device)
    
    # Check if input is already too long
    if len(input_ids[0]) >= CONTEXT_LENGTH:
        return tokenizer.decode(input_ids[0, :CONTEXT_LENGTH].tolist())
    
    with torch.no_grad():
        while len(input_ids[0]) < CONTEXT_LENGTH:
            # Get model's predictions
            output = model(input_ids)
            logits = output[0, -1, :] / temperature
            
            # Apply softmax to get probabilities
            probs = F.softmax(logits, dim=-1)
            
            # Sample from the distribution
            next_token = torch.multinomial(probs, num_samples=1).item()
            
            # Check for game ending tokens
            if next_token in [tokenizer.token_to_id['<WHITE_WINS>'],
                            tokenizer.token_to_id['<BLACK_WINS>'],
                            tokenizer.token_to_id['<DRAW>']]:
                input_ids = torch.cat([input_ids, torch.tensor([[next_token]], device=device)], dim=1)
                break
            
            # Add the token
            input_ids = torch.cat([input_ids, torch.tensor([[next_token]], device=device)], dim=1)

    return tokenizer.decode(input_ids[0].tolist())

#######################
# Main Training Loop
#######################

if __name__ == "__main__":
    # 1. Load pre-processed games
    print("Loading pre-processed games...")
    with open("/content/drive/MyDrive/processed_games_10k.json", "r") as f:
        processed_games = json.load(f)
    print(f"Loaded {len(processed_games)} games")

    # 2. Create Data Loaders
    tokenizer = ChessTokenizer()
    train_loader, val_loader, num_train, num_val = create_dataloaders(processed_games, tokenizer)
    print(f"Created data loaders for {num_train} training samples and {num_val} validation samples")

    # 3. Initialize Model and Load Checkpoint if exists
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"Using device: {device}")

    model = ChessTransformer(
        vocab_size=len(tokenizer.token_to_id),
        d_model=D_MODEL,
        n_heads=N_HEADS,
        n_layers=N_LAYERS,
        dropout=DROPOUT
    ).to(device)

    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)
    
    # Initialize training history
    start_epoch = 0
    train_losses = []
    val_losses = []
    batch_indices = []
    
    # Load checkpoint if it exists
    checkpoint_path = 'chess_model_checkpoint.pt'
    if os.path.exists(checkpoint_path):
        print(f"Loading checkpoint from {checkpoint_path}")
        checkpoint = torch.load(checkpoint_path, map_location=device)
        model.load_state_dict(checkpoint['model_state_dict'])
        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        start_epoch = checkpoint['epoch'] + 1  # Start from next epoch
        train_losses = checkpoint.get('train_losses', [])
        val_losses = checkpoint.get('val_losses', [])
        batch_indices = checkpoint.get('batch_indices', [])
        print(f"Resuming training from epoch {start_epoch}")

    # 4. Training Loop
    print("\nStarting training...")
    for epoch in range(start_epoch, start_epoch + NUM_EPOCHS):
        recent_train_loss, epoch_train_losses, epoch_val_losses, epoch_batch_indices = train_model(
            model, train_loader, val_loader, optimizer, device
        )
        
        # Extend training history
        train_losses.extend(epoch_train_losses)
        val_losses.extend(epoch_val_losses)
        if batch_indices:
            # Adjust batch indices to continue from last checkpoint
            last_batch = batch_indices[-1] if batch_indices else 0
            batch_indices.extend([i + last_batch for i in epoch_batch_indices])
        else:
            batch_indices.extend(epoch_batch_indices)
        
        print(f"\nEpoch {epoch} completed")
        print(f"Recent Train Loss (last 100 batches): {recent_train_loss:.4f}")
        print(f"Final Val Loss: {epoch_val_losses[-1]:.4f}")
        
        print("\nGenerated Game:")
        generated_game = generate_move(model, tokenizer, "<START><WHITE>")
        print(generated_game)
        print("-" * 80)
        
        # Save model checkpoint
        torch.save({
            'epoch': epoch,
            'model_state_dict': model.state_dict(),
            'optimizer_state_dict': optimizer.state_dict(),
            'train_losses': train_losses,
            'val_losses': val_losses,
            'batch_indices': batch_indices,
        }, checkpoint_path)
    
    print("\nTraining complete! Final training history has been saved to 'training_history.png'")